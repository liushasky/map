{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils import *\n",
    "from config import *\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e614872-9e82-48e1-a524-7b6bcae25055",
   "metadata": {},
   "outputs": [],
   "source": [
    "supplementary_data = gpd.read_file('/home/jovyan/work/supplementary_data/data.shp')\n",
    "standard_data = gpd.read_file('/home/jovyan/work/standard_data/data.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8265627-9e5b-4b9a-b5d6-a727a24f49ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geometry_join import *\n",
    "if True:\n",
    "## Explode standard_data\n",
    "        standard_data_cp = standard_data[~pd.isnull(standard_data['geometry'])]\n",
    "        standard_data_cp['geo_linestring_id'] = range(0, standard_data_cp.shape[0])\n",
    "        standard_data_cp['coords'] = standard_data_cp.apply(explode_geometry, axis=1)\n",
    "        standard_data_cp['label'] = 'standard'\n",
    "        standard_data_cp['geo_linestring_id'] = standard_data_cp['label']+'-'+standard_data_cp['geo_linestring_id'].astype(str)\n",
    "        standard_data_exp = standard_data_cp.explode('coords')\n",
    "        standard_data_exp[['lon','lat']] = standard_data_exp.coords.to_list()\n",
    "        standard_data_exp['geo_point_id'] = range(0, standard_data_exp.shape[0])\n",
    "        ## Explode supplementary_data\n",
    "        supplementary_data_cp = supplementary_data[~pd.isnull(supplementary_data['geometry'])]\n",
    "        supplementary_data_cp['geo_linestring_id'] = range(0, supplementary_data_cp.shape[0])\n",
    "        supplementary_data_cp['coords'] = supplementary_data_cp.apply(explode_geometry, axis=1)\n",
    "        supplementary_data_cp['label'] = 'supplementary'\n",
    "        supplementary_data_cp['geo_linestring_id'] = supplementary_data_cp['label']+'-'+supplementary_data_cp['geo_linestring_id'].astype(str)\n",
    "        supplementary_data_exp = supplementary_data_cp.explode('coords')\n",
    "        supplementary_data_exp[['lon','lat']] = supplementary_data_exp.coords.to_list()\n",
    "        supplementary_data_exp['geo_point_id'] = range(0, supplementary_data_exp.shape[0])\n",
    "        ## Set Index\n",
    "        standard_data_exp.reset_index(inplace=True)\n",
    "        standard_data_exp.drop(['index'],axis=1,inplace=True)\n",
    "        supplementary_data_exp.reset_index(inplace=True)\n",
    "        supplementary_data_exp.drop(['index'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d176948-b5a0-449f-90b4-de2f0aad834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree, NearestNeighbors\n",
    "from shapely.geometry.linestring import LineString\n",
    "from shapely.geometry.multilinestring import MultiLineString\n",
    "if True:\n",
    "        knn_model = NearestNeighbors(n_neighbors=1, algorithm='kd_tree', n_jobs=-1)\n",
    "        knn_model.fit(standard_data_exp[['lon','lat']])\n",
    "        supplementary_data_exp['dist_match_point'], supplementary_data_exp['geo_point_id_match_point'] = knn_model.kneighbors(supplementary_data_exp[['lon','lat']], return_distance=True, n_neighbors=1)\n",
    "        supplementary_data_exp['geo_point_id_match_point'] = np.where(supplementary_data_exp['dist_match_point']>0.0004, np.nan, supplementary_data_exp['geo_point_id_match_point'])\n",
    "        supplementary_data_exp['dist_match_point'] = np.where(supplementary_data_exp['dist_match_point']>0.0004, np.nan, supplementary_data_exp['dist_match_point'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geometry_join import LineStringJoin\n",
    "GJ = LineStringJoin(n_neighbors=1, radius_neighbors=0.0004)\n",
    "columns_mapper = {\n",
    "    'geometry':'geometry',\n",
    "    'name':'nom_sec',\n",
    "    'osmid':'cod_tronc',\n",
    "}\n",
    "data_join = GJ.join(sourcedata, targetdata, columns_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88223336-cd39-46b3-9e94-a0682b44075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "osmdata = gpd.read_file('hti_osm_road.zip!hti_osm_road')\n",
    "osmdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-advancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main pipeline file\"\"\"\n",
    "from kubernetes import client as k8s_client\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "\n",
    "@dsl.pipeline(\n",
    "  name='Tacos vs. Burritos',\n",
    "  description='Simple TF CNN'\n",
    ")\n",
    "def tacosandburritos_train(\n",
    "    tenant_id,\n",
    "    service_principal_id,\n",
    "    service_principal_password,\n",
    "    subscription_id,\n",
    "    resource_group,\n",
    "    workspace\n",
    "):\n",
    "  \"\"\"Pipeline steps\"\"\"\n",
    "\n",
    "  persistent_volume_path = '/mnt/azure'\n",
    "  data_download = 'https://aiadvocate.blob.core.windows.net/public/tacodata.zip'\n",
    "  epochs = 5\n",
    "  batch = 32\n",
    "  learning_rate = 0.0001\n",
    "  model_name = 'tacosandburritos'\n",
    "  profile_name = 'tacoprofile'\n",
    "  operations = {}\n",
    "  image_size = 160\n",
    "  training_folder = 'train'\n",
    "  training_dataset = 'train.txt'\n",
    "  model_folder = 'model'\n",
    "\n",
    "  # preprocess data\n",
    "  operations['preprocess'] = dsl.ContainerOp(\n",
    "    name='preprocess',\n",
    "    image='insert image name:tag',\n",
    "    command=['python'],\n",
    "    arguments=[\n",
    "      '/scripts/data.py',\n",
    "      '--base_path', persistent_volume_path,\n",
    "      '--data', training_folder,\n",
    "      '--target', training_dataset,\n",
    "      '--img_size', image_size,\n",
    "      '--zipfile', data_download\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  # train\n",
    "  operations['training'] = dsl.ContainerOp(\n",
    "    name='training',\n",
    "    image='insert image name:tag',\n",
    "    command=['python'],\n",
    "    arguments=[\n",
    "      '/scripts/train.py',\n",
    "      '--base_path', persistent_volume_path,\n",
    "      '--data', training_folder,\n",
    "      '--epochs', epochs,\n",
    "      '--batch', batch,\n",
    "      '--image_size', image_size,\n",
    "      '--lr', learning_rate,\n",
    "      '--outputs', model_folder,\n",
    "      '--dataset', training_dataset\n",
    "    ]\n",
    "  )\n",
    "  operations['training'].after(operations['preprocess'])\n",
    "\n",
    "  # register model\n",
    "  operations['register'] = dsl.ContainerOp(\n",
    "    name='register',\n",
    "    image='insert image name:tag',\n",
    "    command=['python'],\n",
    "    arguments=[\n",
    "      '/scripts/register.py',\n",
    "      '--base_path', persistent_volume_path,\n",
    "      '--model', 'latest.h5',\n",
    "      '--model_name', model_name,\n",
    "      '--tenant_id', tenant_id,\n",
    "      '--service_principal_id', service_principal_id,\n",
    "      '--service_principal_password', service_principal_password,\n",
    "      '--subscription_id', subscription_id,\n",
    "      '--resource_group', resource_group,\n",
    "      '--workspace', workspace\n",
    "    ]\n",
    "  )\n",
    "  operations['register'].after(operations['training'])\n",
    "\n",
    "  operations['deploy'] = dsl.ContainerOp(\n",
    "    name='deploy',\n",
    "    image='insert image name:tag',\n",
    "    command=['sh'],\n",
    "    arguments=[\n",
    "      '/scripts/deploy.sh',\n",
    "      '-n', model_name,\n",
    "      '-m', model_name,\n",
    "      '-i', '/scripts/inferenceconfig.json',\n",
    "      '-d', '/scripts/deploymentconfig.json',\n",
    "      '-t', tenant_id,\n",
    "      '-r', resource_group,\n",
    "      '-w', workspace,\n",
    "      '-s', service_principal_id,\n",
    "      '-p', service_principal_password,\n",
    "      '-u', subscription_id,\n",
    "      '-b', persistent_volume_path\n",
    "    ]\n",
    "  )\n",
    "  operations['deploy'].after(operations['register'])\n",
    "  for _, op_1 in operations.items():\n",
    "    op_1.container.set_image_pull_policy(\"Always\")\n",
    "    op_1.add_volume(\n",
    "      k8s_client.V1Volume(\n",
    "        name='azure',\n",
    "        persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(\n",
    "          claim_name='azure-managed-disk')\n",
    "      )\n",
    "    ).add_volume_mount(k8s_client.V1VolumeMount(\n",
    "      mount_path='/mnt/azure', name='azure'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  compiler.Compiler().compile(tacosandburritos_train, __file__ + '.tar.gz')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ox)",
   "language": "python",
   "name": "ox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
